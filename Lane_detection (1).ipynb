{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c84cd204",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing some useful packages\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import cv2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13e29f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_perspective_transform(img, src, dst, size):\n",
    "    \"\"\" \n",
    "    #---------------------\n",
    "    # This function takes in an image with source and destination image points,\n",
    "    # generates the transform matrix and inverst transformation matrix, \n",
    "    # warps the image based on that matrix and returns the warped image with new perspective, \n",
    "    # along with both the regular and inverse transform matrices.\n",
    "    #\n",
    "    \"\"\"\n",
    "\n",
    "    M = cv2.getPerspectiveTransform(src, dst)\n",
    "    Minv = cv2.getPerspectiveTransform(dst, src)\n",
    "    warp_img = cv2.warpPerspective(img, M, size, flags=cv2.INTER_LINEAR)\n",
    "\n",
    "    return warp_img, M, Minv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a8f0aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def  abs_sobel_thresh(img_channel, orient='x', thresh=(20, 100), sobel_kernel=3):\n",
    "    \"\"\"\n",
    "    Find edges that are aligned vertically and horizontally on the image\n",
    "\n",
    "    :param img_channel: Channel from an image\n",
    "    :param orient: Across which axis of the image are we detecting edges?\n",
    "    :sobel_kernel: No. of rows and columns of the kernel (i.e. 3x3 small matrix)\n",
    "    :return: Image with Sobel edge detection applied\n",
    "    \"\"\"\n",
    "    # cv2.Sobel(input image, data type, prder of the derivative x, order of the\n",
    "    # derivative y, small matrix used to calculate the derivative)\n",
    "    if orient == 'x':\n",
    "    # Will detect differences in pixel intensities going from \n",
    "        # left to right on the image (i.e. edges that are vertically aligned)\n",
    "        sobel =  np.absolute(cv2.Sobel(img_channel, cv2.CV_64F, 1, 0, sobel_kernel))\n",
    "    if orient == 'y':\n",
    "        # Will detect differences in pixel intensities going from \n",
    "        # top to bottom on the image (i.e. edges that are horizontally aligned)\n",
    "        sobel =  np.absolute(cv2.Sobel(img_channel, cv2.CV_64F, 0, 1, sobel_kernel))\n",
    "\n",
    "    # Scale to 8-bit (0 - 255) then convert to type = np.uint8    \n",
    "    scaled_sobel = np.uint8(255*sobel/np.max(sobel))\n",
    "   \n",
    "    # Create a binary mask where mag thresholds are met  \n",
    "    binary_output = np.zeros_like(scaled_sobel)\n",
    "    binary_output[(scaled_sobel >= thresh[0]) & (scaled_sobel <= thresh[1])] = 255\n",
    "\n",
    "    # Return the result\n",
    "    return binary_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b985f23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mag_thresh(img, sobel_kernel=3, mag_thresh=(0, 255)):\n",
    "    \"\"\"\n",
    "    #---------------------\n",
    "    # This function takes in an image and optional Sobel kernel size, \n",
    "    # as well as thresholds for gradient magnitude. And computes the gradient magnitude, \n",
    "    # applies a threshold, and creates a binary output image showing where thresholds were met.\n",
    "    #\n",
    "    \"\"\"\n",
    "    # Take the gradient in x and y separately\n",
    "    sobelx = cv2.Sobel(img, cv2.CV_64F, 1, 0, ksize=sobel_kernel)\n",
    "    sobely = cv2.Sobel(img, cv2.CV_64F, 0, 1, ksize=sobel_kernel)\n",
    "    \n",
    "    # Calculate the gradient magnitude\n",
    "    gradmag = np.sqrt(sobelx**2 + sobely**2)\n",
    "    \n",
    "    # Scale to 8-bit (0 - 255) and convert to type = np.uint8\n",
    "    scale_factor = np.max(gradmag)/255\n",
    "    gradmag = (gradmag/scale_factor).astype(np.uint8)\n",
    "\n",
    "    # Create a binary mask where mag thresholds are met    \n",
    "    binary_output = np.zeros_like(gradmag)\n",
    "    binary_output[(gradmag >= mag_thresh[0]) & (gradmag <= mag_thresh[1])] = 255\n",
    "\n",
    "    # Return the binary image\n",
    "    return binary_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecede13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dir_thresh(img, sobel_kernel=3, thresh=(0.7, 1.3)):\n",
    "    \"\"\"\n",
    "    #---------------------\n",
    "    # This function applies Sobel x and y, \n",
    "    # then computes the direction of the gradient,\n",
    "    # and then applies a threshold.\n",
    "    #\n",
    "    \"\"\"\n",
    "    # Take the gradient in x and y separately\n",
    "    sobelx = cv2.Sobel(img, cv2.CV_64F, 1, 0, ksize=sobel_kernel)\n",
    "    sobely = cv2.Sobel(img, cv2.CV_64F, 0, 1, ksize=sobel_kernel)\n",
    "    \n",
    "    # Take the absolute value of the x and y gradients \n",
    "    # and calculate the direction of the gradient\n",
    "    absgraddir = np.arctan2(np.absolute(sobely), np.absolute(sobelx))\n",
    "   \n",
    "    # Create a binary mask where direction thresholds are met \n",
    "    binary_output = np.zeros_like(absgraddir)\n",
    "    binary_output[(absgraddir >= thresh[0]) & (absgraddir <= thresh[1])] = 255\n",
    "    \n",
    "    # Return the binary image\n",
    "    return binary_output.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bd0fabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_combined_gradients(img, thresh_x, thresh_y, thresh_mag, thresh_dir):\n",
    "    \"\"\"\n",
    "    #---------------------\n",
    "    # This function isolates lane line pixels, by focusing on pixels\n",
    "    # that are likely to be part of lane lines.\n",
    "    # I am using Red Channel, since it detects white pixels very well. \n",
    "    #\n",
    "    \"\"\"\n",
    "    rows, cols = img.shape[:2]\n",
    "    \n",
    "    # save cropped image for documentation\n",
    "    temp = np.copy(img)\n",
    "    temp = temp[:, 0:cols, 2]\n",
    "    cv2.imwrite(\"./output_images/02_cropped.png\", temp)\n",
    "\n",
    "    R_channel = img[:, 0:cols, 2]   # focusing only on regions where lane lines are likely present\n",
    "\n",
    "    sobelx = abs_sobel_thresh(R_channel, 'x', thresh_x)\n",
    "    sobely = abs_sobel_thresh(R_channel, 'y', thresh_y)\n",
    "    mag_binary = mag_thresh(R_channel, 3, thresh_mag)\n",
    "    dir_binary = dir_thresh(R_channel, 15, thresh_dir)\n",
    "    \n",
    "    # debug\n",
    "    #cv2.imshow('sobelx', sobelx)\n",
    "\n",
    "    # combine sobelx, sobely, magnitude & direction measurements\n",
    "    gradient_combined = np.zeros_like(dir_binary).astype(np.uint8)\n",
    "    gradient_combined[((sobelx > 1) & (mag_binary > 1) & (dir_binary > 1)) | ((sobelx > 1) & (sobely > 1))] = 255  # | (R > 1)] = 255\n",
    "\n",
    "    return gradient_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "742d52ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def channel_thresh(channel, thresh=(80, 255)):\n",
    "    \"\"\"\n",
    "    #---------------------\n",
    "    # This function takes in a channel of an image and\n",
    "    # returns thresholded binary image\n",
    "    # \n",
    "    \"\"\"\n",
    "    binary = np.zeros_like(channel)\n",
    "    binary[(channel > thresh[0]) & (channel <= thresh[1])] = 255\n",
    "    return binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "313228d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_combined_hls(img, th_h, th_l, th_s):\n",
    "    \"\"\"\n",
    "    #---------------------\n",
    "    # This function takes in an image, converts it to HLS colorspace, \n",
    "    # extracts individual channels, applies thresholding on them\n",
    "    #\n",
    "    \"\"\"\n",
    "\n",
    "    # convert to hls color space\n",
    "    hls = cv2.cvtColor(img, cv2.COLOR_BGR2HLS)\n",
    "\n",
    "    rows, cols = img.shape[:2]\n",
    "    \n",
    "    # trying to use Red channel info to improve results\n",
    "    #R = img[220:rows - 12, 0:cols, 2]\n",
    "    #_, R = cv2.threshold(R, 180, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    H = hls[:, 0:cols, 0]\n",
    "    L = hls[:, 0:cols, 1]\n",
    "    S = hls[:, 0:cols, 2]\n",
    "\n",
    "    h_channel = channel_thresh(H, th_h)\n",
    "    l_channel = channel_thresh(L, th_l)\n",
    "    s_channel = channel_thresh(S, th_s)\n",
    "    \n",
    "    # debug\n",
    "    #cv2.imshow('Thresholded S channel', s_channel)\n",
    "\n",
    "    # Trying to use Red channel, it works even better than S channel sometimes, \n",
    "    # but in cases where there is shadow on road and road color is different, \n",
    "    # S channel works better. \n",
    "    hls_comb = np.zeros_like(s_channel).astype(np.uint8)\n",
    "    hls_comb[((s_channel > 1) & (l_channel == 0)) | ((s_channel == 0) & (h_channel > 1) & (l_channel > 1))] = 255 \n",
    "    # trying to use both S channel and R channel\n",
    "    #hls_comb[((s_channel > 1) & (h_channel > 1)) | (R > 1)] = 255\n",
    "   \n",
    "    # return combined hls image \n",
    "    return hls_comb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4eda8795",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_grad_hls(grad, hls):\n",
    "    \"\"\" \n",
    "    #---------------------\n",
    "    # This function combines gradient and hls images into one.\n",
    "    # For binary gradient image, if pixel is bright, set that pixel value in reulting image to 255\n",
    "    # For binary hls image, if pixel is bright, set that pixel value in resulting image to 255 \n",
    "    # Edit: Assign different values to distinguish them\n",
    "    # \n",
    "    \"\"\"\n",
    "    result = np.zeros_like(hls).astype(np.uint8)\n",
    "    result[(grad > 1)] = 100\n",
    "    result[(hls > 1)] = 255\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4189de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def region_of_interest(img, vertices):\n",
    "\n",
    "    #defining a blank mask to start with\n",
    "    mask = np.zeros_like(img)   \n",
    "    \n",
    "    #defining a 3 channel or 1 channel color to fill the mask with depending on the input image\n",
    "    if len(img.shape) > 2:\n",
    "        channel_count = img.shape[2]  # i.e. 3 or 4 depending on your image\n",
    "        ignore_mask_color = (255,) * channel_count\n",
    "    else:\n",
    "        ignore_mask_color = 255\n",
    "        \n",
    "    #filling pixels inside the polygon defined by \"vertices\" with the fill color    \n",
    "    cv2.fillPoly(mask, vertices, ignore_mask_color)\n",
    "    \n",
    "    #returning the image only where mask pixels are nonzero\n",
    "    masked_image = cv2.bitwise_and(img, mask)\n",
    "    return masked_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e69817cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_lines(img, lines, color=[255, 0, 0], thickness=10):\n",
    "    # Get the size of the figure\n",
    "    xsize, ysize = img.shape[1], img.shape[0]\n",
    "    \n",
    "    # Fit two linear function for left/right lanes\n",
    "    x_left = []\n",
    "    y_left = []\n",
    "    x_right = []\n",
    "    y_right = []\n",
    "    for line in lines:\n",
    "        for x1, y1, x2, y2 in line:\n",
    "            if abs(x1-x2) == 0 or abs((y1-y2)/(x1-x2)) < 0.5:\n",
    "                # Skip vertical line and the dot \n",
    "                break\n",
    "                \n",
    "            elif (y2-y1)/(x2-x1) > 0:\n",
    "                # Right lane\n",
    "                x_right.append(x2)\n",
    "                x_right.append(x1)\n",
    "                y_right.append(y2)\n",
    "                y_right.append(y1)\n",
    "            else:\n",
    "                # Left lane\n",
    "                x_left.append(x2)\n",
    "                x_left.append(x1)\n",
    "                y_left.append(y2)\n",
    "                y_left.append(y1)\n",
    "                \n",
    "    y_start = ysize//2 + 120\n",
    "    y_end = ysize\n",
    "    if x_left:\n",
    "        k_left, b_left = np.polyfit(x_left, y_left, 1)\n",
    "        x_start_left = int((y_start-b_left)/k_left)\n",
    "        x_end_left = int((y_end-b_left)/k_left)\n",
    "        cv2.line(img, (x_start_left, y_start), (x_end_left, ysize), color, thickness)\n",
    "    if x_right:\n",
    "        k_right, b_right = np.polyfit(x_right, y_right, 1)\n",
    "        x_start_right = int((y_start-b_right)/k_right)\n",
    "        x_end_right = int((y_end-b_right)/k_right)\n",
    "        cv2.line(img, (x_start_right, y_start), (x_end_right, ysize), color, thickness)\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        center_lane = (x_end_right + x_end_left) / 2\n",
    "        lane_width = x_end_right - x_end_left\n",
    "\n",
    "\n",
    "        center_car = xsize / 2\n",
    "        if center_lane > center_car:\n",
    "            deviation = 'Vehicle is '+ str(round(abs(center_lane - center_car)*3.7/lane_width, 3)) + 'm Left of center'\n",
    "        elif center_lane < center_car:\n",
    "            deviation = 'Vehicle is '+ str(round(abs(center_lane - center_car)*3.7/lane_width, 3)) + 'm Right of center'\n",
    "        else:\n",
    "            deviation = 'by 0 (Centered)'\n",
    "\n",
    "        cv2.putText(img, deviation, (10, 63), cv2.FONT_HERSHEY_SIMPLEX, 0.50, (100, 100, 100), 1)\n",
    "    except:pass\n",
    "    \n",
    "    try:\n",
    "        contours = np.array([[x_end_left,ysize], [x_start_left,y_start], [x_start_right,y_start], [x_end_right,ysize]])\n",
    "        cv2.fillPoly(img, pts = [contours], color =(0,200,0))\n",
    "    except:pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d74bf91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hough_lines(img, rho, theta, threshold, min_line_len, max_line_gap):\n",
    "    \n",
    "    lines = cv2.HoughLinesP(img, rho, theta, threshold, np.array([]), minLineLength=min_line_len, maxLineGap=max_line_gap)\n",
    "    line_img = np.zeros((img.shape[0], img.shape[1], 3), dtype=np.uint8)\n",
    "    draw_lines(line_img, lines)\n",
    "    return line_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83eba584",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_img(img, initial_img, α=0.5, β=1., γ=0.):\n",
    "    \"\"\"\n",
    "    `img` is the output of the hough_lines(), An image with lines drawn on it.\n",
    "    Should be a blank image (all black) with lines drawn on it.\n",
    "    \n",
    "    `initial_img` should be the image before any processing.\n",
    "    \n",
    "    The result image is computed as follows:\n",
    "    \n",
    "    initial_img * α + img * β + γ\n",
    "    NOTE: initial_img and img must be the same shape!\n",
    "    \"\"\"\n",
    "    return cv2.addWeighted(initial_img, α, img, β, γ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9581195e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_lane_lines(image):\n",
    "    \n",
    "    xsize, ysize = image.shape[1], image.shape[0]\n",
    "    \n",
    "    th_sobelx, th_sobely, th_mag, th_dir = (35, 100), (30, 255), (30, 255), (0.7, 1.3)\n",
    "    th_h, th_l, th_s = (10, 100), (0, 60), (85, 255)\n",
    "    \n",
    "    combined_gradient = get_combined_gradients(image, th_sobelx, th_sobely, th_mag, th_dir)\n",
    "\n",
    "    combined_hls = get_combined_hls(image, th_h, th_l, th_s)\n",
    "\n",
    "    combined_result = combine_grad_hls(combined_gradient, combined_hls)\n",
    "\n",
    "    low_threshold = 50\n",
    "    high_threshold = 150\n",
    "    edges = cv2.Canny(combined_result, low_threshold, high_threshold)\n",
    "    \n",
    "    vertices = np.array([[(xsize//2-90,ysize//2+120),(100,ysize),(xsize-60, ysize),(xsize//2+130,ysize//2+120)]], dtype=np.int32)\n",
    "    masked_edges = region_of_interest(edges, vertices)\n",
    "    \n",
    "    rho = 1 # distance resolution in pixels of the Hough grid\n",
    "    theta = np.pi/180 # angular resolution in radians of the Hough grid\n",
    "    threshold = 10     # minimum number of votes (intersections in Hough grid cell)\n",
    "    min_line_len = 30 #minimum number of pixels making up a line\n",
    "    max_line_gap = 20   # maximum gap in pixels between connectable line segments\n",
    "    line_img = hough_lines(masked_edges, rho, theta, threshold, min_line_len, max_line_gap)\n",
    "    \n",
    "    img_with_lines = weighted_img(line_img, image, α=0.8, β=1., γ=0.)\n",
    "    \n",
    "    return img_with_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c030f625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import everything needed to edit/save/watch video clips\n",
    "from moviepy.editor import VideoFileClip\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "85dc70d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(image):\n",
    "    \n",
    "    result = draw_lane_lines(image)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a3a784bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t:   0%|                                                                    | 2/1260 [00:00<01:45, 11.94it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video project_video_out2.mp4.\n",
      "Moviepy - Writing video project_video_out2.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready project_video_out2.mp4\n",
      "Wall time: 3min 57s\n"
     ]
    }
   ],
   "source": [
    "white_output = 'project_video_out2.mp4'\n",
    "\n",
    "clip1 = VideoFileClip(\"project_video.mp4\")\n",
    "\n",
    "white_clip = clip1.fl_image(process_image) #NOTE: this function expects color images!!\n",
    "\n",
    "%time white_clip.write_videofile(white_output, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314864d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
